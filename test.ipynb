{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387ac2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1, 2, 3\"\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859c434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62512538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference \n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=\"0\",base_url=\"http://127.0.0.1:8000/v1\")\n",
    "# message = [\n",
    "#     {\n",
    "#     \"role\": \"system\",\n",
    "#     \"content\": (\n",
    "#     \"You are a named-entity recognition assistant. Identify all company names in the following title and text, \"\n",
    "#     \"and output an array of strings, each string being one company name.\"\n",
    "#     )\n",
    "#     },\n",
    "#     {\n",
    "#     \"role\": \"user\",\n",
    "#     \"content\": f\"Title: \\\"Microsoft Acquires GitHub\\\"\\nText: \\\"Microsoft announced today that it has acquired GitHub, and will integrate GitHub Copilot into its Azure cloud platform.\\\"\"\n",
    "#     }\n",
    "# ]\n",
    "message = [{\"role\": \"user\", \"content\":\"who are you?\"}]\n",
    "result = client.chat.completions.create(messages=message, model=\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "print(result.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002125db",
   "metadata": {},
   "source": [
    "batch inference version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ef9089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm # 进度条\n",
    "from openai import OpenAI\n",
    "from openai import APIConnectionError, OpenAIError\n",
    "\n",
    "# 配置 OpenAI-compatible API\n",
    "API_HOST = os.environ.get(\"API_HOST\", \"127.0.0.1\")\n",
    "API_PORT = os.environ.get(\"API_PORT\", \"8520\")\n",
    "openai = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"0\"), base_url=f\"http://{API_HOST}:{API_PORT}/v1\")\n",
    "\n",
    "# 输入/输出文件\n",
    "INPUT_FILE = \"data/clean_data/sft_test.json\"\n",
    "OUTPUT_FILE = \"data/generated/responses.json\"\n",
    "\n",
    "# 最大重试次数 & 延迟\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 1 # 秒\n",
    "SLEEP_BETWEEN = 0.2 # 每条间延迟\n",
    "\n",
    "# 加载所有数据\n",
    "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "    records = json.load(f)\n",
    "\n",
    "results = []\n",
    "\n",
    "# 批量推理\n",
    "for record in tqdm(records, desc=\"Processing records\", unit=\"rec\"):\n",
    "    system = record.get(\"system_prompt\", \"\")\n",
    "    text = record.get(\"prompt\", \"\")\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": system},\n",
    "    {\"role\": \"user\", \"content\": f\"Text: \\\"{text}\\\"\"}\n",
    "    ]\n",
    "\n",
    "# 重试调用\n",
    "    entities = []\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            resp = openai.chat.completions.create(\n",
    "            model=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "            messages=messages,\n",
    "            max_tokens=256,\n",
    "            )\n",
    "            content = resp.choices[0].message.content.strip()\n",
    "            entities = json.loads(content)\n",
    "            break\n",
    "        except APIConnectionError as e:\n",
    "            print(f\"Connection error on attempt {attempt}/{MAX_RETRIES}: {e}\")\n",
    "            time.sleep(RETRY_DELAY)\n",
    "        except OpenAIError as e:\n",
    "            print(f\"API error: {e}\")\n",
    "            break\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Invalid JSON response: {content}\")\n",
    "            break\n",
    "\n",
    "    record[\"predicted_entities\"] = entities\n",
    "    results.append(record)\n",
    "    time.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "# 写出结果\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Batch processing completed: {len(results)} records saved to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd58ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import snapshot_download\n",
    "# model_dir = snapshot_download('Qwen/Qwen2.5-7B-Instruct', cache_dir='/data/mengao/models')\n",
    "# model_dir = snapshot_download('AI-ModelScope/Fin-R1', cache_dir='/data/mengao/models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30ebd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import deepspeed\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "model_dir = '/data/mengao/models/Qwen/Qwen2.5-7B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "model = deepspeed.init_inference(\n",
    "    model,\n",
    "    mp_size=1,\n",
    "    dtype=torch.float16,\n",
    "    replace_with_kernel_inject=True,\n",
    "    min_params_dtype=torch.float16,\n",
    "    replace_method='auto',\n",
    "    tensor_parallel_degree=3,\n",
    ")\n",
    "\n",
    "prompt = \"\"\"\n",
    "<|im_start|>[INST] <<SYS>>\n",
    "You are a named-entity recognition assistant. Identify all company names in the following title and text, and output an array of strings, each string being one company name.\n",
    "<</SYS>>\n",
    "Title: \"Microsoft Acquires GitHub\"\n",
    "Text: \"Microsoft announced today that it has acquired GitHub, and will integrate GitHub Copilot into its Azure cloud platform.\"\n",
    "[/INST]\n",
    "<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "\n",
    "\n",
    "response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f5a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "import time\n",
    "\n",
    "# 配置 OpenAI 或 TGI API\n",
    "openai.api_base = \"http://localhost:8080/v1\" # 如果使用本地 TGI 服务\n",
    "openai.api_key = \"\" # TGI 模式下可留空，或填写你的 OpenAI API Key\n",
    "\n",
    "# 输入和输出文件路径\n",
    "INPUT_FILE = \"input_data.json\"\n",
    "OUTPUT_FILE = \"output_data.json\"\n",
    "\n",
    "# 加载输入数据\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "data = json.load(f)\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, record in enumerate(data):\n",
    "title = record.get(\"title\", \"\")\n",
    "text = record.get(\"text\", \"\")\n",
    "\n",
    "# 构造系统和用户消息\n",
    "messages = [\n",
    "{\n",
    "\"role\": \"system\",\n",
    "\"content\": (\n",
    "\"You are a named-entity recognition assistant. Identify all company names in the following title and text, \"\n",
    "\"and output an array of strings, each string being one company name.\"\n",
    ")\n",
    "},\n",
    "{\n",
    "\"role\": \"user\",\n",
    "\"content\": f\"Title: \\\"{title}\\\"\\nText: \\\"{text}\\\"\"\n",
    "}\n",
    "]\n",
    "\n",
    "# 调用 Chat Completion 接口\n",
    "try:\n",
    "resp = openai.ChatCompletion.create(\n",
    "model=\"gpt-3.5-turbo\",\n",
    "messages=messages,\n",
    "temperature=0,\n",
    "max_tokens=200\n",
    ")\n",
    "# 解析返回值为 JSON\n",
    "content = resp.choices[0].message.content.strip()\n",
    "# 假设模型输出的是 JSON 数组格式\n",
    "entities = json.loads(content)\n",
    "except Exception as e:\n",
    "print(f\"Error at record {idx}: {e}\")\n",
    "entities = []\n",
    "\n",
    "# 将识别结果写入 record\n",
    "record[\"predicted_entities\"] = entities\n",
    "results.append(record)\n",
    "\n",
    "# 避免速率限制\n",
    "time.sleep(0.2)\n",
    "\n",
    "# 保存到输出文件\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Processed {len(results)} records, results saved to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a305ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 模型与分词器路径\n",
    "model_name = \"/data/mengao/models/Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 2. 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    trust_remote_code=True\n",
    ").to(device)  # 将模型移动到 GPU 上\n",
    "\n",
    "model.eval()  # 设置模型为评估模式\n",
    "\n",
    "\n",
    "# 3. 定义测试用例 Prompt\n",
    "prompt = \"\"\"\n",
    "<|im_start|>[INST] <<SYS>>\n",
    "You are a named-entity recognition assistant. Identify all company names in the following title and text, and output an array of strings, each string being one company name.\n",
    "<</SYS>>\n",
    "Title: \"Microsoft Acquires GitHub\"\n",
    "Text: \"Microsoft announced today that it has acquired GitHub, and will integrate GitHub Copilot into its Azure cloud platform.\"\n",
    "[/INST]\n",
    "<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "with torch.no_grad():  # 禁用梯度计算以节省内存\n",
    "    # 4. 编码输入\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=1024,  # 根据模型的最大输入长度调整\n",
    "    ).to(device)  # 将输入移动到 GPU 上\n",
    "\n",
    "    # 5. 生成输出\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,  # 设置生成的最大新令牌数\n",
    "        do_sample=False,    # 为了测试一致性，使用贪心模式\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "\n",
    "# # 3. 构建 text-generation 管道\n",
    "# gen = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model.to('cuda'),  # 将模型移动到 GPU 上\n",
    "#     tokenizer=tokenizer,\n",
    "# )  \n",
    "\n",
    "\n",
    "# 5. 生成并截取模型输出\n",
    "# outputs = gen(\n",
    "#     prompt,\n",
    "#     max_new_tokens=10,\n",
    "#     do_sample=True,    # 为了测试一致性，使用贪心模式\n",
    "#     eos_token_id=tokenizer.eos_token_id\n",
    "# )\n",
    "# 模型返回的 full_output 包含 prompt + 生成；我们截取 prompt 之后的部分\n",
    "# generated = outputs[0][\"generated_text\"][len(prompt):].strip()\n",
    "print(\"Output tokens:\", outputs.shape)\n",
    "print(\"Output text:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "# print(generated)\n",
    "torch.cuda.empty_cache()  # 清理 GPU 内存\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
