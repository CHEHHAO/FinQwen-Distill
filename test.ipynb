{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "387ac2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mengao/conda_envs/finetuning/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1, 2, 3\"\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "859c434e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Tesla V100-SXM2-32GB\n",
      "2.0.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62512538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am Qwen, a large language model created by Alibaba Cloud. I am here to assist with a wide range of tasks, answer questions, and help users generate various types of text, such as articles, stories, poems, and more. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# inference \n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=\"0\", base_url=\"http://127.0.0.1:8520/v1\")\n",
    "message = [\n",
    "    {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": (\n",
    "    \"You are a named-entity recognition assistant. Identify all company names in the following title and text, \"\n",
    "    \"and output an array of strings, each string being one company name.\"\n",
    "    )\n",
    "    },\n",
    "    {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": f\"Title: \\\"Microsoft Acquires GitHub\\\"\\nText: \\\"Microsoft announced today that it has acquired GitHub, and will integrate GitHub Copilot into its Azure cloud platform.\\\"\"\n",
    "    }\n",
    "]\n",
    "# message = [{\"role\": \"user\", \"content\":\"who are you?\"}]\n",
    "result = client.chat.completions.create(messages=message, model=\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "print(result.choices[0].message.content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002125db",
   "metadata": {},
   "source": [
    "batch inference version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52ef9089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing records:   0%|          | 0/30 [00:00<?, ?rec/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing records:  57%|█████▋    | 17/30 [10:47<05:21, 24.73s/rec]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API error: Internal Server Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing records: 100%|██████████| 30/30 [18:52<00:00, 37.76s/rec]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processing completed: 30 records saved to data/generated/responses.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm # 进度条\n",
    "from openai import OpenAI\n",
    "from openai import APIConnectionError, OpenAIError\n",
    "\n",
    "# 配置 OpenAI-compatible API\n",
    "API_HOST = os.environ.get(\"API_HOST\", \"127.0.0.1\")\n",
    "API_PORT = os.environ.get(\"API_PORT\", \"8520\")\n",
    "openai = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"0\"), base_url=f\"http://{API_HOST}:{API_PORT}/v1\")\n",
    "\n",
    "# 输入/输出文件\n",
    "INPUT_FILE = \"data/clean_data/sft_test.json\"\n",
    "OUTPUT_FILE = \"data/generated/responses.json\"\n",
    "\n",
    "# 最大重试次数 & 延迟\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 1 # 秒\n",
    "SLEEP_BETWEEN = 0.2 # 每条间延迟\n",
    "\n",
    "# 加载所有数据\n",
    "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "    records = json.load(f)\n",
    "\n",
    "results = []\n",
    "\n",
    "# 批量推理\n",
    "for record in tqdm(records, desc=\"Processing records\", unit=\"rec\"):\n",
    "    title = record.get(\"title\", \"\")\n",
    "    text = record.get(\"text\", \"\")\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": (\n",
    "    \"You are a named-entity recognition assistant. Identify all company names in the following title and text, \"\n",
    "    \"and output an array of strings, each string being one company name.\"\n",
    "    )},\n",
    "    {\"role\": \"user\", \"content\": f\"Title: \\\"{title}\\\"\\nText: \\\"{text}\\\"\"}\n",
    "    ]\n",
    "\n",
    "# 重试调用\n",
    "    entities = []\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            resp = openai.chat.completions.create(\n",
    "            model=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "            messages=messages,\n",
    "            max_tokens=256,\n",
    "            )\n",
    "            content = resp.choices[0].message.content.strip()\n",
    "            entities = json.loads(content)\n",
    "            break\n",
    "        except APIConnectionError as e:\n",
    "            print(f\"Connection error on attempt {attempt}/{MAX_RETRIES}: {e}\")\n",
    "            time.sleep(RETRY_DELAY)\n",
    "        except OpenAIError as e:\n",
    "            print(f\"API error: {e}\")\n",
    "            break\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Invalid JSON response: {content}\")\n",
    "            break\n",
    "\n",
    "    record[\"predicted_entities\"] = entities\n",
    "    results.append(record)\n",
    "    time.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "# 写出结果\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Batch processing completed: {len(results)} records saved to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd58ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import snapshot_download\n",
    "# model_dir = snapshot_download('Qwen/Qwen2.5-7B-Instruct', cache_dir='/data/mengao/models')\n",
    "# model_dir = snapshot_download('AI-ModelScope/Fin-R1', cache_dir='/data/mengao/models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30ebd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import deepspeed\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "model_dir = '/data/mengao/models/Qwen/Qwen2.5-7B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "model = deepspeed.init_inference(\n",
    "    model,\n",
    "    mp_size=1,\n",
    "    dtype=torch.float16,\n",
    "    replace_with_kernel_inject=True,\n",
    "    min_params_dtype=torch.float16,\n",
    "    replace_method='auto',\n",
    "    tensor_parallel_degree=3,\n",
    ")\n",
    "\n",
    "prompt = \"\"\"\n",
    "<|im_start|>[INST] <<SYS>>\n",
    "You are a named-entity recognition assistant. Identify all company names in the following title and text, and output an array of strings, each string being one company name.\n",
    "<</SYS>>\n",
    "Title: \"Microsoft Acquires GitHub\"\n",
    "Text: \"Microsoft announced today that it has acquired GitHub, and will integrate GitHub Copilot into its Azure cloud platform.\"\n",
    "[/INST]\n",
    "<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "\n",
    "\n",
    "response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f5a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "import time\n",
    "\n",
    "# 配置 OpenAI 或 TGI API\n",
    "openai.api_base = \"http://localhost:8080/v1\" # 如果使用本地 TGI 服务\n",
    "openai.api_key = \"\" # TGI 模式下可留空，或填写你的 OpenAI API Key\n",
    "\n",
    "# 输入和输出文件路径\n",
    "INPUT_FILE = \"input_data.json\"\n",
    "OUTPUT_FILE = \"output_data.json\"\n",
    "\n",
    "# 加载输入数据\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "data = json.load(f)\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, record in enumerate(data):\n",
    "title = record.get(\"title\", \"\")\n",
    "text = record.get(\"text\", \"\")\n",
    "\n",
    "# 构造系统和用户消息\n",
    "messages = [\n",
    "{\n",
    "\"role\": \"system\",\n",
    "\"content\": (\n",
    "\"You are a named-entity recognition assistant. Identify all company names in the following title and text, \"\n",
    "\"and output an array of strings, each string being one company name.\"\n",
    ")\n",
    "},\n",
    "{\n",
    "\"role\": \"user\",\n",
    "\"content\": f\"Title: \\\"{title}\\\"\\nText: \\\"{text}\\\"\"\n",
    "}\n",
    "]\n",
    "\n",
    "# 调用 Chat Completion 接口\n",
    "try:\n",
    "resp = openai.ChatCompletion.create(\n",
    "model=\"gpt-3.5-turbo\",\n",
    "messages=messages,\n",
    "temperature=0,\n",
    "max_tokens=200\n",
    ")\n",
    "# 解析返回值为 JSON\n",
    "content = resp.choices[0].message.content.strip()\n",
    "# 假设模型输出的是 JSON 数组格式\n",
    "entities = json.loads(content)\n",
    "except Exception as e:\n",
    "print(f\"Error at record {idx}: {e}\")\n",
    "entities = []\n",
    "\n",
    "# 将识别结果写入 record\n",
    "record[\"predicted_entities\"] = entities\n",
    "results.append(record)\n",
    "\n",
    "# 避免速率限制\n",
    "time.sleep(0.2)\n",
    "\n",
    "# 保存到输出文件\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Processed {len(results)} records, results saved to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a305ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.51it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 31.75 GiB total capacity; 31.17 GiB already allocated; 30.19 MiB free; 31.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      7\u001b[0m     model_name,\n\u001b[1;32m      8\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 2. 加载模型\u001b[39;00m\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 将模型移动到 GPU 上\u001b[39;00m\n\u001b[1;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# 设置模型为评估模式\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# 3. 定义测试用例 Prompt\u001b[39;00m\n",
      "File \u001b[0;32m/data/mengao/conda_envs/finetuning/lib/python3.10/site-packages/transformers/modeling_utils.py:2952\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2947\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2948\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2949\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2950\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2951\u001b[0m         )\n\u001b[0;32m-> 2952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/mengao/conda_envs/finetuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/mengao/conda_envs/finetuning/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/data/mengao/conda_envs/finetuning/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 797 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/data/mengao/conda_envs/finetuning/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/data/mengao/conda_envs/finetuning/lib/python3.10/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/data/mengao/conda_envs/finetuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 31.75 GiB total capacity; 31.17 GiB already allocated; 30.19 MiB free; 31.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# 1. 模型与分词器路径\n",
    "model_name = \"/data/mengao/models/Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 2. 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    trust_remote_code=True\n",
    ").to(device)  # 将模型移动到 GPU 上\n",
    "\n",
    "model.eval()  # 设置模型为评估模式\n",
    "\n",
    "\n",
    "# 3. 定义测试用例 Prompt\n",
    "prompt = \"\"\"\n",
    "<|im_start|>[INST] <<SYS>>\n",
    "You are a named-entity recognition assistant. Identify all company names in the following title and text, and output an array of strings, each string being one company name.\n",
    "<</SYS>>\n",
    "Title: \"Microsoft Acquires GitHub\"\n",
    "Text: \"Microsoft announced today that it has acquired GitHub, and will integrate GitHub Copilot into its Azure cloud platform.\"\n",
    "[/INST]\n",
    "<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "with torch.no_grad():  # 禁用梯度计算以节省内存\n",
    "    # 4. 编码输入\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=1024,  # 根据模型的最大输入长度调整\n",
    "    ).to(device)  # 将输入移动到 GPU 上\n",
    "\n",
    "    # 5. 生成输出\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,  # 设置生成的最大新令牌数\n",
    "        do_sample=False,    # 为了测试一致性，使用贪心模式\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "\n",
    "# # 3. 构建 text-generation 管道\n",
    "# gen = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model.to('cuda'),  # 将模型移动到 GPU 上\n",
    "#     tokenizer=tokenizer,\n",
    "# )  \n",
    "\n",
    "\n",
    "# 5. 生成并截取模型输出\n",
    "# outputs = gen(\n",
    "#     prompt,\n",
    "#     max_new_tokens=10,\n",
    "#     do_sample=True,    # 为了测试一致性，使用贪心模式\n",
    "#     eos_token_id=tokenizer.eos_token_id\n",
    "# )\n",
    "# 模型返回的 full_output 包含 prompt + 生成；我们截取 prompt 之后的部分\n",
    "# generated = outputs[0][\"generated_text\"][len(prompt):].strip()\n",
    "print(\"Output tokens:\", outputs.shape)\n",
    "print(\"Output text:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "# print(generated)\n",
    "torch.cuda.empty_cache()  # 清理 GPU 内存\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
